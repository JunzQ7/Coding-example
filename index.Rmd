---
title: "Coding Example"
author: "Juhana Rautavirta"
date: "30 9 2021"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
In this document I will give some examples of the code that I have been writing lately. The vast majority of the coding that I have been writing the past months is written in R, so I will include only R code here. Most of the code is from my Master's thesis, which is about validating a Bayesian predictive method used in amphetamine profiling. However, I will not include any results that are obtained with real data (only simulated).

## 1. Simulation
I will now simulate some data that can be used for some of the analysis that I have done in my thesis. Let us imagine the following situation: we are comparing two amphetamine samples $x$ and $y$. The following question arises: what is the probability that $x$ and $y$ origin from the same laboratory? Let us also imagine that there is a method $f$, which gives the answer to this question, i.e, $f(x,y) = P(x\text{ and } y\text{ origin from the same lab}) \in [0,1]$. Now imagine that we have data of $3000$ sample pairs, so the data is in the form $(x_i,y_i), \text{ where }i=1,\dots3000$. Let us also have the information that pairs $i=1,\dots,1000$ come from the same lab, pairs $i=1001,\dots,2000$ do not but they are somehow linked to each other and $i=2001,\dots 3000$ have absolutely nothing to do with each other. 

We will now simulate the results that the method $f$ could have given for the data. As stated before these results will be probabilities, so they must be in the unit interval $[0,1]$. A suitable distribution for this simulation is the Beta-distribution, so let us simulate 1000 data points from each $Beta(\alpha_i, \beta_i)$ with 
$$
\begin{aligned}
(\alpha_1,\beta_1) &= (6,1) \\
(\alpha_2,\beta_2) &= (2,2) \\
(\alpha_3,\beta_3) &= (1,5)
\end{aligned}
$$

```{r}
n = 1000
alpha_1 = 6
alpha_2 = 2
alpha_3 = 1
beta_1 = 1
beta_2 = 2
beta_3 = 5

simulatedData = data.frame("Same" = rbeta(n, alpha_1, beta_1),
                           "Linked" = rbeta(n, alpha_2, beta_2),
                           "Random" = rbeta(n, alpha_3, beta_3))
head(simulatedData)
```

## 2.  Analysis

Let us now try to analyze how the method $f$ works on the data that we simulated in the previous section. Drawing overlapping density plots is a nice way to visualize the situation and to get a overview of the performance of the method.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
```


```{r}
# define the plotting function 
plotMultipleDensities <- function(df, value, label, 
                                 title, xlim = c(0,1), includeLegend = TRUE) {
  x_label = 'f(x,y)'
  plt <- ggplot(df, aes(x=eval(parse(text=value)), fill = eval(parse(text=label)))) +
    geom_density(alpha=.7) +
    labs(x=x_label, y = "Density") +
    xlim(xlim) + 
    theme_minimal() + 
    ggtitle(title)
  if(!includeLegend) (plt = plt+theme(legend.position = "none"))
  return(plt + guides(fill=guide_legend(title=label)))
}

# gather the data in a functional form
gatheredData = gather(simulatedData, "Label", "Value", 1:3)

# use the plotting function
plotMultipleDensities(gatheredData, value = "Value", label = "Label", title = "Overlapping density plots")
```

For this simulated data, one could say that the method can distinguish the "Random" and the "Same" pairs fairly well, but the "Linked" sample pairs can get mixed up with both of the other types.

Since the values $f(x,y)$ are probabilities, it is very tempting (and justified) to assess the performance of the method with the ROC curve (receiver operating characteristic curve). The ROC curve is basically the true positive rate (tpr) plotted against the false positive fate (fpr).
Let us think about the sample pairs labeled by "Random" as a positive class and sample pairs labeled by "Same" as a negative class. Now if we think about the method as a binary classifier, we can compute a ROC curve of the performance of $f$ for the simulated data.

```{r}
library(PRROC)
```

```{r}
# compute tpr and fpr
computeROC = function(df) {
  rocObject = roc.curve(df$Same, df$Random, curve = T)
  fprTprData = data.frame(rocObject$curve[,1:2])
  colnames(fprTprData) = c("fpr", "tpr")
  return(list(fprTprData = fprTprData,
              auc = rocObject$auc))
}
```



```{r}
# function to plot ROC curves
plotROC = function(df, title, auc) {
  plt <- ggplot(df, aes(x=fpr, y=tpr)) +
    geom_point() +
    theme_minimal() +
    ggtitle(title) +
    annotate("text", x=.5, y=.5,
             label = paste0("AUC = ", round(auc,3))) +
    annotate("rect", xmin = .375, xmax = .625, ymin = .375, 
             ymax = .625, alpha = .2, colour = "red")
  return(plt)
}

plotROC(computeROC(simulatedData)$fprTprData, title = "ROC curve",
        auc = computeROC(simulatedData)$auc)
```

The AUC value 0.998 has the following interpretation: the probability that a randomly drawn sample from the negative class will get a lower score (probability in our case) than a randomly drawn sample from the positive class, is $99.8\%$, which is obviously very high.

## 3. Discussion

This analysis was a very harsh example of what I have done in my thesis. What I did not include in this document was for example the repeated k-fold cross-validation as one validation method. I have also used the AUC of the Precision-Recall curve as a performance measure, but it is not included in this document (as for now).
